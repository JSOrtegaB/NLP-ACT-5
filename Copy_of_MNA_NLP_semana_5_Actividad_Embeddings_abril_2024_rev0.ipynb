{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Adtividad Semana 5\n",
        "\n",
        "### **Vectores Embebidos Pre-entrenados: Fasttext**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   Elemento de lista\n",
        "*   Elemento de lista\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCL2p6MA8NuT",
        "outputId": "55a227b8-54a8-4b9b-a0c1-36e3c0c2a1aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sortega/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/sortega/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/sortega/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "##**Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
        "\n",
        "**Llama simplemente \"df\" a dicho DataFrame.**\n",
        "\n",
        "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_lyEFRkxzC6",
        "outputId": "381f71c3-17a4-478a-d767-39a8dadbdc0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Define the relative paths to the files\n",
        "amazon_file_path = os.path.join(current_dir, 'amazon5.txt')\n",
        "imdb_file_path = os.path.join(current_dir, 'imdb5.txt')\n",
        "yelp_file_path = os.path.join(current_dir, 'yelp5.txt')\n",
        "\n",
        "# Reading the files\n",
        "amazon_df = pd.read_csv(amazon_file_path, sep='\\t', header=None, names=['review', 'label'])\n",
        "imdb_df = pd.read_csv(imdb_file_path, sep=r'\\s{3}', header=None, names=['review', 'label'], engine='python')\n",
        "yelp_df = pd.read_csv(yelp_file_path, sep='\\t', header=None, names=['review', 'label'])\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate the three DataFrames into one\n",
        "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-w1xMLYnm9b",
        "outputId": "861bfb6e-49e6-445f-be2a-fb8d9f959108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ],
      "source": [
        "# Verifiquemos la información del DataFrame:\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NfVUcYe1nubT",
        "outputId": "466972d2-2924-4a9f-ea3a-51362c01e442"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  So there is no way for me to plug it in here i...      0\n",
              "1                        Good case, Excellent value.      1\n",
              "2                             Great for the jawbone.      1\n",
              "3  Tied to charger for conversations lasting more...      0\n",
              "4                                  The mic is great.      1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Y veamos sus primeros registros:\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "##**Pregunta - 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6JF5BommZ6"
      },
      "source": [
        "Realiza el proceso de limpieza.\n",
        "\n",
        "Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TsnvMp-7oYCM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "mystopwords = set(stopwords.words('english')).difference(negwords)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_repeated_characters(text):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "def clean_and_lemmatize(text):\n",
        "    # Normalize repeated characters\n",
        "    normalized_text = normalize_repeated_characters(text)\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(normalized_text)\n",
        "    # Remove stopwords, lemmatize, and remove short tokens\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in tokens\n",
        "        if word.lower() not in mystopwords and word.isalpha() and len(word) > 1\n",
        "    ]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "\n",
        "X = df['review']\n",
        "Y = df['label'].astype(int)\n",
        "\n",
        "Xclean = X.apply(clean_and_lemmatize)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jlQuoI2o33T",
        "outputId": "d10e1be3-376d-4bdb-cf9a-ea10cc4895a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "great camera thats pic nice clear great picture quality\n",
            "not impressed product\n",
            "nice headset priced right\n",
            "hear garbage audio\n",
            "excellent bluetooth headset\n"
          ]
        }
      ],
      "source": [
        "# Despleguemos los primeros comentarios después de tu proceso de limpieza:\n",
        "\n",
        "for x in Xclean[40:45]:\n",
        "  print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygchEdcKqIzU"
      },
      "source": [
        "#**Pregunta - 3:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEIOkkl9Dot"
      },
      "source": [
        "\n",
        "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0SAcYdq9X0w",
        "outputId": "3c3307ab-f0b1-4b9a-97a1-e4f4c7d3fc85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n",
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ************* Inicia la sección de agregar código:*****************************\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))\n",
        "\n",
        "\n",
        "# *********** Termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# verificemos las dimensiones obtenidas:\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "#**Pregunta - 4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jENsKiN99r3F"
      },
      "source": [
        "\n",
        "\n",
        "Construye tu vocabulario a continuación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzJntmLPqPqC",
        "outputId": "cf401ee7-e812-4b8f-8445-785690be86fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del diccionario: 3578\n",
            "\n",
            "(word,frequency):\n",
            "[('not', 225), ('good', 160), ('movie', 140), ('great', 137), ('phone', 131), ('film', 125), ('time', 100), ('one', 99), ('like', 88), ('work', 86)]\n"
          ]
        }
      ],
      "source": [
        "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
        "#     con un tamaño que consideres adecuado:\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "midiccionario = Counter()\n",
        "\n",
        "for text in x_train:\n",
        "    tokens = [token for token in text.split() if token.strip()]\n",
        "    midiccionario.update(tokens)\n",
        "\n",
        "\n",
        "print('Longitud del diccionario:', len(midiccionario))\n",
        "print('\\n(word,frequency):')\n",
        "print(midiccionario.most_common(10))\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTDZ0Rr86CUP",
        "outputId": "51f09052-5a2c-4851-ae05-94e3ef1eccb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del vocabulario generado:\n",
            "3578\n"
          ]
        }
      ],
      "source": [
        "# b.\tIndica el tamaño del vocabulario generado.\n",
        "\n",
        "print('Longitud del vocabulario generado:')\n",
        "\n",
        "\n",
        "# ******* Inicia la sección de agregar código: ***********\n",
        "\n",
        "\n",
        "# Imprimir la longitud del vocabulario generado\n",
        "print(len(midiccionario))\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDa4EhTqrw15"
      },
      "source": [
        "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "*Para evitar Data leakegae. De esta forma aseguramos que el modelo generalize bien y no se aprenda la información*\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ykjxQI3rpxx",
        "outputId": "b8e0db4f-3549-46aa-ad87-aac3a39173f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario filtrado: 1506\n"
          ]
        }
      ],
      "source": [
        "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
        "#     validación y prueba para que todos los comentarios usen solamente las\n",
        "#     palabras de este vocabulario.\n",
        "\n",
        "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "def filtrar_comentarios(conjunto, vocabulario):\n",
        "    comentarios_filtrados = []\n",
        "    for comentario in conjunto:\n",
        "        tokens = [token for token in comentario.split() if token in vocabulario]\n",
        "        comentarios_filtrados.append(' '.join(tokens))\n",
        "    return comentarios_filtrados\n",
        "\n",
        "numero_minimo_ocurrencias = 2\n",
        "\n",
        "# Filtrar las palabras en el vocabulario\n",
        "vocabulario_filtrado = {word for word, count in midiccionario.items() if count >= numero_minimo_ocurrencias}\n",
        "\n",
        "print('Tamaño del vocabulario filtrado:', len(vocabulario_filtrado))\n",
        "\n",
        "# Aplicar la función a los conjuntos de entrenamiento, validación y prueba\n",
        "train_x = filtrar_comentarios(x_train, vocabulario_filtrado)\n",
        "val_x = filtrar_comentarios(x_val, vocabulario_filtrado)\n",
        "test_x = filtrar_comentarios(x_test, vocabulario_filtrado)\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYF2RGuPtQTC",
        "outputId": "c2258f8a-0097-4a44-89d4-db4ef60b9af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fare much better people like morgan ed wasted\n",
            "tonight filet sucked\n",
            "paid bill not tip felt server terrible job\n",
            "call ca properly cook steak understand\n",
            "however keypad tinny sometimes wrong button\n"
          ]
        }
      ],
      "source": [
        "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
        "\n",
        "for ss in train_x[0:5]:\n",
        "  print(ss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "#**Pregunta - 5:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnHHAza5_P5Z"
      },
      "source": [
        "\n",
        "a. Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTI9xSgF_Xc8"
      },
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqRl7fT_fn2"
      },
      "source": [
        "#**Pregunta - 6:**\n",
        "\n",
        "Utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300.\n",
        "\n",
        "Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
        "\n",
        "Es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo.\n",
        "\n",
        "Recuerda borrar la variable donde descargaste los 2 millones de vectores embebidos Fasttext.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdK-jMfLxHLY",
        "outputId": "0873c06d-3e0f-437f-b414-334a7cb6396d"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAJXqiuJUPnu",
        "outputId": "206a4a42-5711-4e2a-8867-de31bc1cf096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario filtrado: 1506\n",
            "No se encontró el vector embebido para la palabra \"razr\"\n",
            "No se encontró el vector embebido para la palabra \"huston\"\n",
            "No se encontró el vector embebido para la palabra \"jabra\"\n",
            "No se encontró el vector embebido para la palabra \"errol\"\n",
            "No se encontró el vector embebido para la palabra \"plantronics\"\n",
            "No se encontró el vector embebido para la palabra \"schrader\"\n",
            "No se encontró el vector embebido para la palabra \"miyazaki\"\n",
            "No se encontró el vector embebido para la palabra \"vinegrette\"\n",
            "No se encontró el vector embebido para la palabra \"fulci\"\n",
            "No se encontró el vector embebido para la palabra \"mishima\"\n",
            "No se encontró el vector embebido para la palabra \"stanwyck\"\n",
            "Tamaño del diccionario de vectores embebidos: 1495\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Tamaño del vocabulario filtrado:', len(vocabulario_filtrado))\n",
        "embedding_dict = {}\n",
        "\n",
        "# Para cada palabra en el vocabulario filtrado, obtener su vector embebido\n",
        "for word in vocabulario_filtrado:\n",
        "    if word in fasttext_model:\n",
        "        embedding_dict[word] = fasttext_model[word]\n",
        "    else:\n",
        "        print(f'No se encontró el vector embebido para la palabra \"{word}\"')\n",
        "\n",
        "print(f'Tamaño del diccionario de vectores embebidos: {len(embedding_dict)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5u3U0JxUens",
        "outputId": "a651ad8f-b2b8-437c-905f-31eed9a8e8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diccionario de vectores embebidos guardado en embedding_dict.pkl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Guardar el diccionario en un archivo\n",
        "with open('embedding_dict.pkl', 'wb') as file:\n",
        "    pickle.dump(embedding_dict, file)\n",
        "\n",
        "print('Diccionario de vectores embebidos guardado en embedding_dict.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sDXdKCfuUgWY"
      },
      "outputs": [],
      "source": [
        "del fasttext_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4S7q0yR0Mpi"
      },
      "source": [
        "#**Pregunta - 7:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyeOrkoaC1eq"
      },
      "source": [
        "\n",
        "\n",
        "Generamos los vectores embebidos a paertir de los conjuntos de entrenamiento, validación y preuba.\n",
        "\n",
        "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnfQpkxg0Usq",
        "outputId": "d4366a28-1fba-4134-dcc4-56b093ddfe44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diccionario de vectores embebidos guardado en embedding_dict.pkl\n",
            "Vectores embebidos guardados en archivos pkl.\n"
          ]
        }
      ],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "with open('embedding_dict.pkl', 'wb') as file:\n",
        "    pickle.dump(embedding_dict, file)\n",
        "\n",
        "print('Diccionario de vectores embebidos guardado en embedding_dict.pkl')\n",
        "\n",
        "# Generate embedding vectors function with padding\n",
        "def generar_vectores_embebidos(conjunto, embedding_dict, embedding_dim=300, max_length=None):\n",
        "    vectores_embebidos = []\n",
        "    for comentario in conjunto:\n",
        "        tokens = comentario.split()\n",
        "        vector_comentario = []\n",
        "        for token in tokens:\n",
        "            if token in embedding_dict:\n",
        "                vector_comentario.append(embedding_dict[token])\n",
        "            else:\n",
        "                vector_comentario.append(np.zeros(embedding_dim))\n",
        "        \n",
        "        # Pad sequences to the maximum length\n",
        "        if max_length:\n",
        "            while len(vector_comentario) < max_length:\n",
        "                vector_comentario.append(np.zeros(embedding_dim))\n",
        "            vector_comentario = vector_comentario[:max_length]\n",
        "        \n",
        "        vectores_embebidos.append(np.array(vector_comentario))\n",
        "    return np.array(vectores_embebidos)\n",
        "\n",
        "# Determine the maximum length of the sequences\n",
        "max_length = max(len(comentario.split()) for comentario in train_x + val_x + test_x)\n",
        "\n",
        "# Generate embedding vectors for the datasets with padding\n",
        "trainEmb = generar_vectores_embebidos(train_x, embedding_dict, max_length=max_length)\n",
        "valEmb = generar_vectores_embebidos(val_x, embedding_dict, max_length=max_length)\n",
        "testEmb = generar_vectores_embebidos(test_x, embedding_dict, max_length=max_length)\n",
        "\n",
        "# Save the embedding vectors to files\n",
        "with open('trainEmb.pkl', 'wb') as file:\n",
        "    pickle.dump(trainEmb, file)\n",
        "with open('valEmb.pkl', 'wb') as file:\n",
        "    pickle.dump(valEmb, file)\n",
        "with open('testEmb.pkl', 'wb') as file:\n",
        "    pickle.dump(testEmb, file)\n",
        "\n",
        "print('Vectores embebidos guardados en archivos pkl.')\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "J3BBF96D0N8Z",
        "outputId": "0c00f688-b898-4cd5-87d6-3953205cdb68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Emb: (2100, 22, 300)\n",
            "Val-Emb: (450, 22, 300)\n",
            "Test-Emb: (450, 22, 300)\n"
          ]
        }
      ],
      "source": [
        "# Veamos las dimensiones de cada conjunto embebido:\n",
        "\n",
        "print(\"Train-Emb:\", trainEmb.shape)\n",
        "print(\"Val-Emb:\", valEmb.shape)\n",
        "print(\"Test-Emb:\", testEmb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibp1LA91CP_"
      },
      "source": [
        "#**Pregunta - 8:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxC9K0VnGOwG"
      },
      "source": [
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
        "\n",
        "Compara los resultados con los de la semana anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ycwjD8ztGOL7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regresión Logística - Desempeño en el conjunto de entrenamiento\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93      1068\n",
            "           1       0.94      0.91      0.92      1032\n",
            "\n",
            "    accuracy                           0.92      2100\n",
            "   macro avg       0.92      0.92      0.92      2100\n",
            "weighted avg       0.92      0.92      0.92      2100\n",
            "\n",
            "Regresión Logística - Desempeño en el conjunto de validación\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.80      0.79       216\n",
            "           1       0.81      0.79      0.80       234\n",
            "\n",
            "    accuracy                           0.80       450\n",
            "   macro avg       0.80      0.80      0.80       450\n",
            "weighted avg       0.80      0.80      0.80       450\n",
            "\n",
            "Regresión Logística - Desempeño en el conjunto de prueba\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       216\n",
            "           1       0.83      0.81      0.82       234\n",
            "\n",
            "    accuracy                           0.81       450\n",
            "   macro avg       0.81      0.81      0.81       450\n",
            "weighted avg       0.81      0.81      0.81       450\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# REGRESIÓN LOGÍSTICA:\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Aplanar las matrices de vectores embebidos\n",
        "X_train = np.array([x.flatten() for x in trainEmb])\n",
        "X_val = np.array([x.flatten() for x in valEmb])\n",
        "X_test = np.array([x.flatten() for x in testEmb])\n",
        "\n",
        "# Entrenar el modelo de regresión logística\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predecir y evaluar\n",
        "y_pred_train = logreg.predict(X_train)\n",
        "y_pred_val = logreg.predict(X_val)\n",
        "y_pred_test = logreg.predict(X_test)\n",
        "\n",
        "print(\"Regresión Logística - Desempeño en el conjunto de entrenamiento\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Regresión Logística - Desempeño en el conjunto de validación\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "print(\"Regresión Logística - Desempeño en el conjunto de prueba\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N4n70GHW0sl3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bosque Aleatorio - Desempeño en el conjunto de entrenamiento\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00      1068\n",
            "           1       0.99      1.00      1.00      1032\n",
            "\n",
            "    accuracy                           1.00      2100\n",
            "   macro avg       1.00      1.00      1.00      2100\n",
            "weighted avg       1.00      1.00      1.00      2100\n",
            "\n",
            "Bosque Aleatorio - Desempeño en el conjunto de validación\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.78      0.74       216\n",
            "           1       0.77      0.69      0.73       234\n",
            "\n",
            "    accuracy                           0.73       450\n",
            "   macro avg       0.74      0.74      0.73       450\n",
            "weighted avg       0.74      0.73      0.73       450\n",
            "\n",
            "Bosque Aleatorio - Desempeño en el conjunto de prueba\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.77      0.73       216\n",
            "           1       0.76      0.68      0.72       234\n",
            "\n",
            "    accuracy                           0.72       450\n",
            "   macro avg       0.73      0.73      0.72       450\n",
            "weighted avg       0.73      0.72      0.72       450\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# BOSQUE ALEATORIO (Random Forest):\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predecir y evaluar\n",
        "y_pred_train_rf = rf.predict(X_train)\n",
        "y_pred_val_rf = rf.predict(X_val)\n",
        "y_pred_test_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"Bosque Aleatorio - Desempeño en el conjunto de entrenamiento\")\n",
        "print(classification_report(y_train, y_pred_train_rf))\n",
        "\n",
        "print(\"Bosque Aleatorio - Desempeño en el conjunto de validación\")\n",
        "print(classification_report(y_val, y_pred_val_rf))\n",
        "\n",
        "print(\"Bosque Aleatorio - Desempeño en el conjunto de prueba\")\n",
        "print(classification_report(y_test, y_pred_test_rf))\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDIiSHvg0_hm"
      },
      "source": [
        "#**Pregunta - 9:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJtALGZHrGk"
      },
      "source": [
        "\n",
        "\n",
        "Reporte del mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ETv4VLjP1GYt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparación de Resultados\n",
            "                Modelo  Precisión    Recall  F1-Score\n",
            "0                LR S4   0.850000  0.840000  0.840000\n",
            "1                RF S4   0.820000  0.810000  0.810000\n",
            "2                NB S4   0.820000  0.810000  0.810000\n",
            "3  Regresión Logística   0.813755  0.813333  0.813400\n",
            "4     Bosque Aleatorio   0.728740  0.724444  0.724161\n"
          ]
        }
      ],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "previous_results = {\n",
        "    'Modelo': ['LR S4', 'RF S4', 'NB S4'],\n",
        "    'Precisión': [0.85, 0.82, 0.82],\n",
        "    'Recall': [0.84, 0.81, 0.81],\n",
        "    'F1-Score': [0.84, 0.81, 0.81]\n",
        "}\n",
        "\n",
        "# Resultados actuales para regresión logística\n",
        "logreg_results = {\n",
        "    'Modelo': ['Regresión Logística'],\n",
        "    'Precisión': [classification_report(y_test, y_pred_test, output_dict=True)['weighted avg']['precision']],\n",
        "    'Recall': [classification_report(y_test, y_pred_test, output_dict=True)['weighted avg']['recall']],\n",
        "    'F1-Score': [classification_report(y_test, y_pred_test, output_dict=True)['weighted avg']['f1-score']]\n",
        "}\n",
        "\n",
        "# Resultados actuales para bosque aleatorio\n",
        "rf_results = {\n",
        "    'Modelo': ['Bosque Aleatorio'],\n",
        "    'Precisión': [classification_report(y_test, y_pred_test_rf, output_dict=True)['weighted avg']['precision']],\n",
        "    'Recall': [classification_report(y_test, y_pred_test_rf, output_dict=True)['weighted avg']['recall']],\n",
        "    'F1-Score': [classification_report(y_test, y_pred_test_rf, output_dict=True)['weighted avg']['f1-score']]\n",
        "}\n",
        "\n",
        "# Convertir resultados a DataFrames\n",
        "previous_df = pd.DataFrame(previous_results)\n",
        "logreg_df = pd.DataFrame(logreg_results)\n",
        "rf_df = pd.DataFrame(rf_results)\n",
        "\n",
        "# Combinar todos los resultados\n",
        "comparison_df = pd.concat([previous_df, logreg_df, rf_df], axis=0, ignore_index=True)\n",
        "\n",
        "# Mostrar la comparación\n",
        "print(\"Comparación de Resultados\")\n",
        "print(comparison_df)\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkh2WfN1MC1"
      },
      "source": [
        "#**Pregunta - 10:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySFuDQtVuK5"
      },
      "source": [
        "\n",
        "\n",
        "Incluye tus comentarios finales de la actividad.\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKHmQTbWJT1"
      },
      "source": [
        "##**Fin de la Actividad de vectores Embebidos - FastText**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
